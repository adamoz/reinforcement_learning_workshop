{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reinforcement_Learning_Workshop_Solutions.ipynb","provenance":[],"collapsed_sections":["llcYRAlNqwQ_","ldQKxtjJngsU","icQggVN4D6Xm","jS-ktf1SEpZ3","_7R6duW-1ubs","4KlLzshaKhUy","TRaBdJQF-jyQ","u_XhQwBM8tAt","owTltl5TBTmI","ZY9XzYfEC-Wo","86tnqR0DRrV9","4jYBG2wJl9GO","9oqaRKC8rlUR","k3FwWn4IvAgI","Y0g2nhwKo6W9","xtJBmHvOD51J","E5ptvAy7IOlI","-YZHRXMC0ax0","RhA13OzL21KZ","f7Qum86dTKpL","LQxxQn-bTQJO","2vLWqoveTo58","-2WOMfunW53Y","t2YqeC17ZaPG","9nz61CWqhRDL","VMPu8R25ArTd","JukkY6-xky5L","N44AsCp6Lump","Dl5HUVeOxZB7","UBvSeOIuNWGc","vRzZJsv4YJc9","JPQVpjTidM6q","ccCHYqnR4cMY","lvFSwWbsdedX","ezgWQ74xeLdh","_B7s8AaeeWAj","L5GfRHKWe6LO","WeTeQOFTfcvg","R2Yl7MffgIEN","5225GpdlkMul","frwVZbloCDQu","7N1Hn0-n-uVv","ybV1tZdjFX_7","n0dfyX0a-e5p","gDZE0y91lkUk","HPQR5BCKdNPp","TxEhS1tIsBxs","Xnf4dX_T4416","153oUaq6E4Yh"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XsIVlmkrnTBB"},"source":["#Reinfrocement Learning Workshop\n","---"]},{"cell_type":"markdown","metadata":{"id":"llcYRAlNqwQ_"},"source":["## Imports & Installs"]},{"cell_type":"code","metadata":{"id":"ju5EbEQzWMMU"},"source":["!apt install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install box2d-py\n","!pip install gym==0.17.2 > /dev/null 2>&1\n","!pip install pyvirtualdisplay==1.3.2 > /dev/null 2>&1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJX-1qmhgo0W"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2avvFmJaYG_"},"source":["import gym\n","import glob\n","import base64\n","import io\n","import os\n","import sys\n","import tqdm\n","import numpy as np\n","import copy\n","import random\n","import torch\n","import time\n","import pandas as pd\n","import seaborn as sns\n","import pyvirtualdisplay\n","from gym.wrappers import Monitor\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML\n","from collections import defaultdict\n","from pyvirtualdisplay import Display\n","from collections import namedtuple, deque\n","from torch.distributions import Categorical\n","from IPython import display as ipythondisplay"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqTgBJD1sNyf"},"source":["# Setup of xvfb display server wrapper.\n","display = Display(visible=0, size=(400, 300))\n","display.start()\n","os.environ.get(\"DISPLAY\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMtAX3BOm4GV"},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video(path_prefix='/video/'):\n","  if not path_prefix.endswith('/'):\n","    path_prefix += '/'\n","  mp4list = glob.glob(f'{path_prefix}*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    fmt = 'ascii'\n","    ipythondisplay.display(HTML(\n","        data=f'<video alt=\"recording\" autoplay loop controls style=\"height: 400px;\">\\\n","               <source src=\"data:video/mp4;base64,{encoded.decode(fmt)}\" type=\"video/mp4\"/>\\\n","               </video>'))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env, path_prefix='/video/'):\n","  if not path_prefix.endswith('/'):\n","    path_prefix += '/'\n","  env = Monitor(env, path_prefix, force=True)\n","  return env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3--r_dPBqCtL"},"source":["# Mounting gdrive..\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!mkdir -p  '/content/drive/My Drive/ml_college_data/rl_workshop/'\n","rl_workshop_path = '/content/drive/My Drive/ml_college_data/rl_workshop/'\n","!ls '/content/drive/My Drive/ml_college_data'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldQKxtjJngsU"},"source":["## OpenAI Gym\n"," - introduction of OpenAI gym environment\n"," - exploring simple and more complex environments\n"," - how to visualize game play"]},{"cell_type":"markdown","metadata":{"id":"YmNKwMBaLIej"},"source":["All environments have its leaderboards and defined success scores: https://github.com/openai/gym/wiki/Leaderboard#"]},{"cell_type":"code","metadata":{"id":"lVBu-Ty9nxSw"},"source":["np.random.choice(list(gym.envs.registry.all()), 5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"icQggVN4D6Xm"},"source":["#### Simple text based environment exploration\n","\n","Text environment with fully observable state space. More details can be found at https://gym.openai.com/envs/FrozenLake-v0/\n","\n"]},{"cell_type":"code","metadata":{"id":"7q-5lvpjS6t0"},"source":["# Creating and resetting of the gym environment..\n","game = \"FrozenLake8x8-v0\"\n","env = gym.make(game)\n","state = env.reset()\n","screen = env.render(mode='ansi')\n","print(screen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m8IEfK3AXGjt"},"source":["env.observation_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwon4hprP90m"},"source":["np.arange(64).reshape((8, 8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0m76LkUzUw_U"},"source":["env.action_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SLaAmcEORZh"},"source":["env.action_space.sample()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2vfnoA8jxw-_"},"source":["# Semantic of operations.\n","LEFT = 0\n","DOWN = 1\n","RIGHT = 2\n","UP = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zo1mCuS_Ym6A"},"source":["state = 1\n","action = RIGHT\n","env.P[state][action]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lm7Cl4v4agjI"},"source":["# Exploration of non-deterministic behaviour of environment.\n","next_state, reward, done, info = env.step(action)\n","screen = env.render(mode='ansi')\n","print(screen)\n","print('Action:', action)\n","print('Next state:', next_state)\n","print('Reward:', reward)\n","print('Done:', done)\n","print('Info:', info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xnNAxVh6RUot"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bd_uUNLb6Fh"},"source":["#############\n","# Game play #\n","#############\n","\n","\n","# Setup of environment.\n","game = \"FrozenLake8x8-v0\"\n","env = gym.make(game)\n","state = env.reset()\n","\n","for iteration in range(100):\n","  # Sampling random action.\n","  action = env.action_space.sample()\n","  # Applying action in the envrinment.\n","  next_state, reward, done, info = env.step(action)\n","  # Clearing up the screen.\n","  ipythondisplay.clear_output(wait=True)\n","  screen = env.render('ansi')\n","  print(screen)\n","  print()\n","  print('Action', action)\n","  print('Obs', next_state)\n","  print('Reward', reward)\n","  print('Done', done)\n","  print('Info', info)\n","  time.sleep(0.3)\n","  if done:\n","    print(f\"Agent end up in {iteration} iterations.\")\n","    break\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jS-ktf1SEpZ3"},"source":["#### Complex Atari environment\n","Not fully observable environment of Pacman (https://gym.openai.com/envs/MsPacman-v0/) with even more rich domain of actions and continuous state space."]},{"cell_type":"code","metadata":{"id":"3f709B4a2AMw"},"source":["env = gym.make(\"MsPacman-v0\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51J-JFmD5wUs"},"source":["env.observation_space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYqywb4v2EGn"},"source":["env.action_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTi1tILL4hBo"},"source":["env.get_action_meanings()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"umQAPYE862ui"},"source":["state = env.reset()\n","state.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uXHrTo2N6_QN"},"source":["plt.figure(figsize=(7, 7))\n","f = plt.imshow(env.render('rgb_array'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tj4pG0EDpEpb"},"source":["#############\n","# Game play #\n","#############\n","\n","\n","# Prefix where to save video saved from gameplay\n","prefix = '/video/pacman/play_1/'\n","env = wrap_env(gym.make(\"MsPacman-v0\"), prefix)\n","\n","state = env.reset()\n","while True:\n","    # 'human' type of rendenring is suitable when we do the recording.\n","    env.render('human')\n","    action = env.action_space.sample()      \n","    next_state, reward, done, info = env.step(action) \n","    if done: \n","      break;   \n","env.close()\n","\n","show_video(prefix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_7R6duW-1ubs"},"source":["#### Questions and experiment suggestions\n"," - Explore other environments, eg. https://gym.openai.com/envs/CartPole-v1/. There is always a link to actual code of environment to check details about behaviour etc.."]},{"cell_type":"markdown","metadata":{"id":"4KlLzshaKhUy"},"source":["## Model Based Methods\n"," - revisit of frozen lake environment\n"," - policy evaluation in the fully observable environment\n"," - policy improvement and interation\n"," - visualization of the V and policy"]},{"cell_type":"markdown","metadata":{"id":"TRaBdJQF-jyQ"},"source":["#### Frozen Lake revisited"]},{"cell_type":"code","metadata":{"id":"2_9lMdk_9sqn"},"source":["game = \"FrozenLake-v0\"\n","env = gym.make(game, map_name=\"4x4\", is_slippery=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_R-ucik-quf"},"source":["env.action_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nHAj2LO7-jI7"},"source":["LEFT = 0\n","DOWN = 1\n","RIGHT = 2\n","UP = 3\n","\n","policy_mapping = {\n","    0: \"LEFT\",\n","    1: \"DOWN\",\n","    2: \"RIGHT\",\n","    3: \"UP\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NlvhT3iU-ux8"},"source":["env.observation_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0mDtumoDW3v"},"source":["state = 10\n","print(env.P[state][LEFT])\n","print()\n","prob, next_state, reward, done = env.P[state][LEFT][0]\n","prob, next_state, reward, done"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0TBHwcb-pW-"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hak_LTp8TEzr"},"source":["# Comparison of non deterministic and deterministic environment.\n","env = gym.make(game, map_name=\"4x4\", is_slippery=False)\n","state = 10\n","env.P[state][LEFT]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXPmxuxVn-AS"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_XhQwBM8tAt"},"source":["#### Policy evaluation"]},{"cell_type":"code","metadata":{"id":"jmHjOInS81Oe"},"source":["game = \"FrozenLake-v0\"\n","env = gym.make(game, map_name=\"4x4\", is_slippery=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rISVkwGcx_Hv"},"source":["policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n","policy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTvVeyb-_N8-"},"source":["def policy_evaluation(policy, env, gamma=0.9, improvement=1e-8):\n","  \"\"\" Applies policy to environment and collects results to build value function.\n","      \n","      Params:\n","        policy: np.array([states, actions]).\n","        env: OpneAI envirnment.\n","        gamma: Discount of future rewards.\n","        improvements: Minimal improvement of V to continue evaluation.\n","      Return: Value function.\n","  \"\"\"\n","\n","  V = np.zeros(env.observation_space.n)\n","\n","  while True:\n","    # Record biggest single iteration improvement.\n","    delta = 0\n","    # Iterate over all states, actions and env responses to actions.\n","    for state in range(env.observation_space.n):\n","      v_s = 0\n","      for action in env.P[state].keys():\n","        for prob, next_state, reward, done in env.P[state][action]:\n","          v_s += policy[state][action] * prob * (reward + gamma * V[next_state])\n","      # Keep information about biggest improvement.\n","      delta = max(delta, np.abs(V[state] - v_s))\n","      V[state] = v_s\n","    if delta < improvement:\n","      break\n","  return V"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Jje2XSk80Fd"},"source":["# Test your policy evaluation method\n","# game = \"FrozenLake-v0\"\n","# env = gym.make(game, map_name=\"4x4\", is_slippery=False)\n","policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n","\n","V_expected = np.array([0.0045, 0.0042, 0.0101, 0.0041, 0.0067, 0., 0.0263, 0., 0.0187, 0.0576, 0.107, 0., 0., 0.1304, 0.3915, 0.])\n","np.testing.assert_array_equal(V_expected, policy_evaluation(policy, env, gamma=0.9, improvement=1e-8).round(4))\n","# env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GTO8EE9MHZ2K"},"source":["V = policy_evaluation(policy, env, gamma=0.9, improvement=1e-8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEooQ0M1H_gp"},"source":["def visualize_V(V, shape, figsize=(10, 10)):\n","  V_df = pd.DataFrame(V.reshape(shape))\n","  plt.figure(figsize=figsize)\n","  fig = sns.heatmap(V_df, annot=True, linewidths=.5, cmap='Blues',  xticklabels=False, yticklabels=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WFqOUiq2DgmO"},"source":["visualize_V(V, (4,4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mm1WN7KGBN_B"},"source":["env.render()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"owTltl5TBTmI"},"source":["#### Policy improvement"]},{"cell_type":"code","metadata":{"id":"rl7j6GofI2He"},"source":["def get_Q(V, env, gamma=0.9):\n","    \"\"\" Calculates Q(s,a) value fucntion based on V and MDP. \"\"\" \n","    Q = np.zeros((env.observation_space.n, env.action_space.n))\n","    for state in range(env.observation_space.n):\n","      for action in range(env.action_space.n):\n","        for prob, next_state, reward, done in env.P[state][action]:\n","            Q[state][action] += prob * (reward + gamma * V[next_state])\n","    return Q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s_cmeYF9BbkL"},"source":["# Test your get_Q fucntion\n","# game = \"FrozenLake-v0\"\n","# env = gym.make(game, map_name=\"4x4\", is_slippery=False)\n","\n","V_test = np.array([0.0045, 0.0042, 0.0101, 0.0041, 0.0067, 0., 0.0263, 0., 0.0187, 0.0576, 0.107, 0., 0., 0.1304, 0.3915, 0.])\n","Q_expected = np.array([\n","       [0.004, 0.006, 0.004, 0.004],\n","       [0.004, 0.   , 0.009, 0.004],\n","       [0.004, 0.024, 0.004, 0.009],\n","       [0.009, 0.   , 0.004, 0.004],\n","       [0.006, 0.017, 0.   , 0.004],\n","       [0.   , 0.   , 0.   , 0.   ],\n","       [0.   , 0.096, 0.   , 0.009],\n","       [0.   , 0.   , 0.   , 0.   ],\n","       [0.017, 0.   , 0.052, 0.006],\n","       [0.017, 0.117, 0.096, 0.   ],\n","       [0.052, 0.352, 0.   , 0.024],\n","       [0.   , 0.   , 0.   , 0.   ],\n","       [0.   , 0.   , 0.   , 0.   ],\n","       [0.   , 0.117, 0.352, 0.052],\n","       [0.117, 0.352, 1.   , 0.096],\n","       [0.   , 0.   , 0.   , 0.   ]])\n","\n","\n","np.testing.assert_array_equal(Q_expected, get_Q(V_test, env, gamma=0.9).round(3))\n","# env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAojF-8FnVyb"},"source":["Q = get_Q(V, env)\n","Q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJTPht2TB484"},"source":["# Check state-action values.\n","state = 0\n","Q[state][DOWN]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ths4xdH9RvzM"},"source":["def policy_improvement(V, env, gamma=0.9):\n","    \"\"\"Generate new policy based on max values for each state in Q\"\"\"\n","    policy = np.zeros([env.observation_space.n, env.action_space.n])\n","    Q = get_Q(V, env, gamma)\n","    for state in range(env.observation_space.n):\n","        # There can be multiple maximal options in Q[state].\n","        best_actions = np.argwhere(Q[state]==np.max(Q[state])).flatten()\n","        # We distribute probability of action evenly amongst maximum values.\n","        policy[state] = np.sum([np.eye(env.nA)[a] for a in best_actions], axis=0) / len(best_actions)\n","    return policy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BaxiP01DmAN"},"source":["policy = policy_improvement(V, env)\n","policy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TciwV403V80d"},"source":["def visualize_policy(policy, shape):\n","  policy_mapping = {\n","    0: \"LEFT\",\n","    1: \"DOWN\",\n","    2: \"RIGHT\",\n","    3: \"UP\"\n","  }\n","  readable_policy = []\n","  for state in range(policy.shape[0]):\n","    # In case of multiple maximas we take first letter of each maximal action.\n","    possible_moves = list(np.argwhere(policy[state] == np.max(policy[state])).flatten())\n","    code = \"\".join(map(lambda move: policy_mapping[move][0], possible_moves))\n","    readable_policy.append(code)\n","  return pd.DataFrame(np.array(readable_policy).reshape(shape))\n","\n","visualize_policy(policy, (4, 4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsmLRZa0pP5a"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZY9XzYfEC-Wo"},"source":["#### Policy iteration"]},{"cell_type":"code","metadata":{"id":"xEmdN6xGepb4"},"source":["game = \"FrozenLake-v0\"\n","env = gym.make(game, map_name=\"8x8\", is_slippery=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w5Id9G1agxd6"},"source":["env.render()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mEVZJ3TmMMZq"},"source":["np.arange(64).reshape((8,8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGWP8bE5ndEk"},"source":["# Policy iteration algorithm\n","policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n","improvement = 1e-3\n","gamma = 0.9\n","\n","for iteration in tqdm.tqdm(range(1000), total=1000):\n","  # Calculate V function based on current policy.\n","  V = policy_evaluation(policy, env, improvement=improvement, gamma=gamma)\n","  # Improve policy based on new value function.\n","  new_policy = policy_improvement(V, env)\n","  if np.all(policy == new_policy) and iteration > 5:\n","    break\n","  policy = new_policy.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2lsTCG_dpC9O"},"source":["visualize_V(V, (8, 8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oO7X2msIb9u0"},"source":["visualize_policy(policy, shape=(8,8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FGR5r4XN55dL"},"source":["policy_mapping"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U473mSeW5Lnp"},"source":["env.P[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QcTyDJa27IK1"},"source":["np.arange(64).reshape((8,8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3sBB9sbgOzIq"},"source":["def get_action(state, policy, action_space_size):\n","  return np.random.choice(action_space_size, size=1, p=policy[state])[0]\n","\n","get_action(6, policy, env.action_space.n)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fCuJRg-oprUg"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XGUYWKQpnRYM"},"source":["#############\n","# Game play #\n","#############\n","\n","\n","game = \"FrozenLake-v0\"\n","env = gym.make(game, map_name=\"8x8\", is_slippery=False)\n","state = env.reset()\n","\n","for iteration in range(100):\n","  # Sampling random action from policy.\n","  action = get_action(state, policy, env.action_space.n)\n","  # Applying action in the envrinment.\n","  next_state, reward, done, info = env.step(action)\n","\n","  ipythondisplay.clear_output(wait=True)\n","  screen = env.render('ansi')\n","  print(screen)\n","  print()\n","  print('Action', action)\n","  print('Obs', next_state)\n","  print('Reward', reward)\n","  print('Done', done)\n","  print('Info', info)\n","  time.sleep(0.3)\n","  if done:\n","    print(f\"We finished after {iteration} iterations\")\n","    break\n","  state = next_state  \n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iS3cXaNI9Tug"},"source":["game = \"FrozenLake-v0\"\n","env = gym.make(game, map_name=\"8x8\", is_slippery=True)\n","\n","cumulative_reward = 0\n","for _ in tqdm.tqdm(range(1000), total=1000):\n","  state = env.reset()\n","  for iteration in range(100):\n","    # Sampling random action from policy.\n","    action = get_action(state, policy, env.action_space.n)\n","    # Applying action in the envrinment.\n","    next_state, reward, done, info = env.step(action)\n","    if done:\n","      break\n","    state = next_state\n","  cumulative_reward += reward  \n","env.close()\n","\n","print()\n","print('Cumulative reward is', cumulative_reward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"86tnqR0DRrV9"},"source":["#### Questions and experiment suggestions\n"," - Change environment to `is_slippery=True` and explore final policy, use `env.P` to understand strange moves agent does.\n"," - Try policy trained in slippery env in non-slippery one and vice versa\n"," - Write function which calculates cumulative reward of policy in slippery env over multiple runs. "]},{"cell_type":"markdown","metadata":{"id":"4jYBG2wJl9GO"},"source":["## Model Free Value Based Methods - Intro\n","Methods exploring environment without knowledge of `env.P` or state size.\n"," - monte carlo policy evaluation\n"," - monte carlo control\n"," - temporad differece learning\n"," - q learning"]},{"cell_type":"markdown","metadata":{"id":"9oqaRKC8rlUR"},"source":["#### Monte Carlo policy evaluation\n","We don't know how many states the environment has, neither the `env.P`"]},{"cell_type":"code","metadata":{"id":"r6TTSWw4rmOR"},"source":["game = \"FrozenLake-v0\"\n","env = gym.make(game, map_name=\"8x8\", is_slippery=False)\n","\n","# Defalut policy for unknow state is random.\n","policy = defaultdict(lambda: np.ones(env.action_space.n) / env.action_space.n)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__9gMLW2re-f"},"source":["def sample_episode_with_policy(env, policy):\n","  \"\"\"Samples the environment with given policy and return whole episode\n","     Return: List of tuples in form (state, action, reward)\n","  \"\"\"\n","  episode = []\n","  state = env.reset()\n","\n","  while True:\n","    # Sampling random action.\n","    action = get_action(state, policy, env.action_space.n)\n","    # Applying action to the envrinment.\n","    next_state, reward, done, info = env.step(action)\n","    episode.append((state, action, reward))\n","    if done:\n","      break\n","    state = next_state  \n","  return episode"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"76ENO41F2njT"},"source":["episode = sample_episode_with_policy(env, policy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMv42zqS48lW"},"source":["states, actions, rewards = zip(*episode)\n","states[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYyERRhu29uM"},"source":["def monte_carlo_policy_evaluation(env, policy, episodes=100000, gamma=0.9):\n","    V_sum = defaultdict(lambda: 0)\n","    V = defaultdict(lambda: 0)\n","    N = defaultdict(lambda: 1)\n","    \n","    for _ in tqdm.tqdm(range(episodes), total=episodes):\n","        episode = sample_episode_with_policy(env, policy)\n","        states, actions, rewards = zip(*episode)\n","        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n","        # For each sampled state we just take the future rewards.\n","        for i, state in enumerate(states):\n","            V_sum[state] += sum(rewards[i:]*discounts[:-(1+i)])\n","            N[state] += 1.0\n","            V[state] = V_sum[state] / N[state]\n","    return V"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pmi894CmIbZe"},"source":["V_88_fully_observed = np.array([\n","       0.25418658, 0.28242954, 0.3138106 , 0.34867844, 0.38742049,\n","       0.43046721, 0.4782969 , 0.531441  , 0.28242954, 0.3138106 ,\n","       0.34867844, 0.38742049, 0.43046721, 0.4782969 , 0.531441  ,\n","       0.59049   , 0.3138106 , 0.34867844, 0.38742049, 0.        ,\n","       0.4782969 , 0.531441  , 0.59049   , 0.6561    , 0.34867844,\n","       0.38742049, 0.43046721, 0.4782969 , 0.531441  , 0.        ,\n","       0.6561    , 0.729     , 0.3138106 , 0.34867844, 0.38742049,\n","       0.        , 0.59049   , 0.6561    , 0.729     , 0.81      ,\n","       0.28242954, 0.        , 0.        , 0.59049   , 0.6561    ,\n","       0.729     , 0.        , 0.9       , 0.3138106 , 0.        ,\n","       0.4782969 , 0.531441  , 0.        , 0.81      , 0.        ,\n","       1.        , 0.34867844, 0.38742049, 0.43046721, 0.        ,\n","       0.81      , 0.9       , 1.        , 0.        ])\n","\n","\n","mses = []\n","for ep in [100, 1000, 10000]:\n","  V = monte_carlo_policy_evaluation(env, policy, episodes=ep, gamma=0.9)\n","  V_88_mc = np.zeros(64)\n","  for state, value in V.items():\n","    V_88_mc[state] = value\n","  mse = np.mean(np.power(V_88_fully_observed - V_88_mc, 2))\n","  mses.append(mse)\n","\n","print()\n","print(mses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qXYlLtBIdzO"},"source":["visualize_V(V_88_mc, shape=(8,8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9lZU9uVXhVSb"},"source":["visualize_V(V_88_fully_observed, shape=(8,8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SX0GJNZjwIh0"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k3FwWn4IvAgI"},"source":["#### Monte Carlo Control  "]},{"cell_type":"code","metadata":{"id":"UhbiByqB5Wgl"},"source":["game = \"FrozenLake-v0\"\n","env = gym.make(game, map_name=\"8x8\", is_slippery=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pjn-5lWS2yuS"},"source":["def get_policy_Qs(Qs, action_space_size, eps=0.99):\n","  \"\"\"Get eps greedy policy for given state based of Q value.\"\"\"\n","  policy = np.ones(action_space_size) * eps / action_space_size\n","  max_action_position = np.argwhere(Qs==np.max(Qs)).flatten()\n","  policy[max_action_position] = (1 - eps) / len(max_action_position) + eps / action_space_size\n","  return policy\n","\n","def get_action_Q(state, Q, action_space_size, eps=0.99):\n","  \"\"\"Sampling action based on Q using epsilon greedy policy\"\"\"\n","  if state not in Q:\n","    return np.random.choice(np.arange(action_space_size))\n","  return np.random.choice(np.arange(action_space_size), p=get_policy_Qs(Q[state], action_space_size, eps))\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecXtK01O4E2l"},"source":["get_policy_Qs(np.array([1,2,3,2,3]), action_space_size=5, eps=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T30Tp3wUwVho"},"source":["def sample_episode_with_Q(env, Q, eps=0.99):\n","  episode = []\n","  state = env.reset()\n","\n","  while True:\n","    action = get_action_Q(state, Q, env.action_space.n, eps)\n","    next_state, reward, done, info = env.step(action)\n","    episode.append((state, action, reward))\n","    if done:\n","      break\n","    state = next_state  \n","  return episode"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhP9QStC9Mh0"},"source":["Q = defaultdict(lambda: np.zeros(env.action_space.n))\n","episode = sample_episode_with_Q(env, Q, eps=0.99)\n","episode[:4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_mVt10ljyBS"},"source":["def monte_carlo_control(env, episodes, alpha=0.01, gamma=0.9, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n","    \"\"\"Monte carlo sampling of environment and improving Q with eps-greedy policy\n","       Params:\n","          alpha: Approxiation of 1/N term in monte carlo policy evaluaton.\n","          gamma: Future reward discount.\n","          eps: How much we we explore rather than exploit env.\n","          eps_decay: Decrease eps each iteration.\n","      Return: Estimated Q function\n","    \"\"\"\n","\n","    action_space_size = env.action_space.n\n","    Q = defaultdict(lambda: np.zeros(action_space_size))\n","    eps = eps_start\n","    \n","    # In each iteration we sample episode and  update Q and policy\n","    for _ in tqdm.tqdm(range(episodes), total=episodes):\n","        episode = sample_episode_with_Q(env, Q, eps)\n","        states, actions, rewards = zip(*episode)\n","        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n","        # Using same discount strategy as with mc policy evaluation.\n","        for i, state in enumerate(states):\n","            cumulative_reward = sum(rewards[i:]*discounts[:-(1+i)])\n","            action = actions[i]\n","            # Q average value with just approximation of N.\n","            Q[state][action] = Q[state][action] + alpha * (cumulative_reward - Q[state][action])\n","        # Decrease eps\n","        eps = max(eps*eps_decay, eps_min)\n","    return Q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HbOH90j95blZ"},"source":["Q = monte_carlo_control(env, episodes=25000, eps_start=0.5, alpha=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBPa5uK3nRGr"},"source":["# For visualization we cheat with prior about size of states.\n","policy_q = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n","for state, Qs in Q.items():\n","  policy_q[state] = get_policy_Qs(Qs, env.action_space.n, eps=0)\n","visualize_policy(policy_q, shape=(8, 8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JjUzRykLERGS"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DJg5e7_i56YY"},"source":["#############\n","# Game play #\n","#############\n","\n","\n","game = \"FrozenLake-v0\"\n","env = gym.make(game, map_name=\"8x8\", is_slippery=False)\n","state = env.reset()\n","\n","for iteration in range(100):\n","  # Sampling random action.\n","  action = get_action_Q(state, Q, env.action_space.n, eps=0.1)\n","  # Applying action in the envrinment.\n","  next_state, reward, done, info = env.step(action)\n","\n","  ipythondisplay.clear_output(wait=True)\n","  screen = env.render('ansi')\n","  print(screen)\n","  print()\n","  print('Action', action)\n","  print('Obs', next_state)\n","  print('Reward', reward)\n","  print('Done', done)\n","  print('Info', info)\n","  time.sleep(0.05)\n","  if done:\n","    print(f\"We finished after {iteration} iterations\")\n","    break\n","  state = next_state  \n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0g2nhwKo6W9"},"source":["#### Questions and experiment suggestions\n"," - Change environment to `is_slippery=True` and find sample count which produce reasonable policy\n"," - Write function calculating cumulative reward of multiple mc-control runs  with given policy and different *eps*"]},{"cell_type":"code","metadata":{"id":"PvtvH12NGM47"},"source":["game = \"FrozenLake-v0\"\n","env = gym.make(game, map_name=\"8x8\", is_slippery=False)\n","\n","cumulative_reward = 0\n","eps = 0.2\n","for _ in tqdm.tqdm(range(1000), total=1000):\n","  state = env.reset()\n","  for iteration in range(100):\n","    # Sampling random action.\n","    action = get_action_Q(state, Q, env.action_space.n, eps=eps)\n","    # Applying action in the envrinment.\n","    next_state, reward, done, info = env.step(action)\n","    if done:\n","      break\n","    state = next_state\n","  cumulative_reward += reward  \n","env.close()\n","\n","print()\n","print('Cumulative reward is', cumulative_reward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xtJBmHvOD51J"},"source":["#### Temporal Difference"]},{"cell_type":"code","metadata":{"id":"MS_yRG7QP_48"},"source":["game = \"FrozenLake-v0\"\n","env = gym.make(game, map_name=\"8x8\", is_slippery=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYXu-_aHIb5a"},"source":["def td_sarsa(env, episodes, alpha=0.01, gamma=0.9, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n","    \"\"\"Temporal difference SARSA apprach with eps greedy policy to estimate Q.\n","       Params:\n","          alpha: Approxiation of 1/N term in monte carlo policy evaluaton.\n","          gamma: Future reward discount.\n","          eps: How much we we explore rather than exploit env.\n","          eps_decay: Decrease eps each iteration.\n","      Return: Estimated Q function\n","    \"\"\"\n","    action_space_size = env.action_space.n\n","    eps = eps_start\n","    Q = defaultdict(lambda: np.zeros(action_space_size))\n","\n","    for _ in tqdm.tqdm(range(episodes), total=episodes):\n","        eps = max(eps*eps_decay, eps_min)\n","        # We sample every step, not waiting for the finish of the episode.\n","        state = env.reset()\n","        action = get_action_Q(state, Q, env.action_space.n, eps)\n","        # Here starts one episode.\n","        while True:\n","            next_state, reward, done, info = env.step(action)\n","            if not done:\n","                next_action = get_action_Q(next_state, Q, env.action_space.n, eps)\n","                # We just estimate cummulative reward based on sarsa sample and current Q.\n","                td_target = reward + gamma * Q[next_state][next_action]\n","                td_error = td_target - Q[state][action]\n","                Q[state][action] += alpha * td_error\n","            # We can not sample next step in case we finished.\n","            else:\n","                td_target = reward\n","                td_error = td_target - Q[state][action]\n","                Q[state][action] += alpha * td_error\n","                break\n","            state = next_state\n","            action = next_action  \n","    return Q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-dPs-k1-QfDY"},"source":["Q = td_sarsa(env, episodes=75000, alpha=0.01, eps_start=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HBj3vk5CQvVT"},"source":["policy_q = np.ones([64, env.action_space.n]) / env.action_space.n\n","for state, Qs in Q.items():\n","  policy_q[state] = get_policy_Qs(Qs, env.action_space.n, eps=0)\n","visualize_policy(policy_q, shape=(8, 8))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E5ptvAy7IOlI"},"source":["#### Questions and experiment suggestions\n"," - Use function for calculating cumulative reward and compare MC, SARSA and Q-learning (next part)"]},{"cell_type":"code","metadata":{"id":"GrzhiEPPRLYM"},"source":["cumulative_reward = 0\n","for _ in tqdm.tqdm(range(1000), total=1000):\n","  state = env.reset()\n","  for iteration in range(100):\n","    # Sampling random action.\n","    action = get_action_Q(state, Q, env.action_space.n, eps=0)\n","    # Applying action in the envrinment.\n","    next_state, reward, done, info = env.step(action)\n","    if done:\n","      break\n","    state = next_state\n","  cumulative_reward += reward  \n","  env.close()\n","\n","print('Cumulative reward is', cumulative_reward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-YZHRXMC0ax0"},"source":["#### Q learining control (off policy)\n","Learinig from optimal policy while following exploratory policy."]},{"cell_type":"code","metadata":{"id":"OgO9D6Ur0c5o"},"source":["def Q_learning(env, episodes, alpha=0.01, gamma=0.9, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n","    \"\"\"Q learning apprach with eps greedy policy to estimate Q.\n","       Params:\n","          alpha: Approxiation of 1/N term in monte carlo policy evaluaton.\n","          gamma: Future reward discount.\n","          eps: How much we we explore rather than exploit env.\n","          eps_decay: Decrease eps each iteration.\n","      Return: Estimated Q function\n","    \"\"\"\n","    action_space_size = env.action_space.n\n","    eps = eps_start\n","    Q = defaultdict(lambda: np.zeros(action_space_size))\n","\n","    for _ in tqdm.tqdm(range(episodes), total=episodes):\n","        eps = max(eps*eps_decay, eps_min)\n","        # We now sample evry step, not after finishing the episode.\n","        state = env.reset()\n","        # Here starts one episode.\n","        while True:\n","            action = get_action_Q(state, Q, env.action_space.n, eps)\n","            next_state, reward, done, info = env.step(action)\n","            if not done:\n","                # We just estimate cummulative reward based on sars sample and current Q.\n","                td_target = reward + gamma * max(Q[next_state])\n","                td_error = td_target - Q[state][action]\n","                Q[state][action] += alpha * td_error\n","            # We can not sample next step in case we finished.\n","            else:\n","                td_target = reward\n","                td_error = td_target - Q[state][action]\n","                Q[state][action] += alpha * td_error\n","                break\n","            state = next_state\n","    return Q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRF_MxYb1L6M"},"source":["Q = Q_learning(env, episodes=25000, alpha=0.01, eps_start=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6BK-5M-z2LoD"},"source":["policy_q = np.ones([64, env.action_space.n]) / env.action_space.n\n","for state, Qs in Q.items():\n","  policy_q[state] = get_policy_Qs(Qs, env.action_space.n, eps=0)\n","visualize_policy(policy_q, shape=(8, 8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5LrFqWn1_39"},"source":["cumulative_reward = 0\n","for _ in tqdm.tqdm(range(1000), total=1000):\n","  state = env.reset()\n","  for iteration in range(100):\n","    # Sampling random action.\n","    action = get_action_Q(state, Q, env.action_space.n, eps=0)\n","    # Applying action in the envrinment.\n","    next_state, reward, done, info = env.step(action)\n","    if done:\n","      break\n","    state = next_state\n","  cumulative_reward += reward  \n","  env.close()\n","\n","print('Cumulative reward is', cumulative_reward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RhA13OzL21KZ"},"source":["## Model Free Value Based Methods - Deep Q-Learning\n"," - lunar environment\n"," - replay buffer\n"," - qnetwork architecture\n"," - QAgent \n"," - deep qlearning algorithm\n"]},{"cell_type":"markdown","metadata":{"id":"f7Qum86dTKpL"},"source":["#### Environment exploration\n","More details at https://gym.openai.com/envs/LunarLander-v2/ and in to charts."]},{"cell_type":"code","metadata":{"id":"gf50hjzXMol2"},"source":["game = \"LunarLander-v2\"\n","env = gym.make(game)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GC30gEEUm5KV"},"source":["env.observation_space.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kdx1X6nm7UJ"},"source":["env.action_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dGN8uNB3mBNJ"},"source":["lunar_actions = {\n","    'none': 0,\n","    'left': 1,\n","    'up': 2,\n","    'right': 3\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"goKbCJWkmG0-"},"source":["state = env.reset()\n","state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qlsy1bF7tvTL"},"source":["plt.figure(figsize=(7,7))\n","next_state, reward, done, info = env.step(lunar_actions['left'])\n","plt.imshow(env.render('rgb_array'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jq4Td87r5cWF"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_dSzTx4Ic5Ab"},"source":["logging_path = '/video/lunar/test_1/'\n","env = wrap_env(gym.make(\"LunarLander-v2\"), logging_path)\n","state = env.reset()\n","\n","while True:\n","    # 'human' type of rendenring is suitable when we do the recording\n","    env.render('human')\n","    action = env.action_space.sample()      \n","    state, reward, done, info = env.step(action) \n","    if done: \n","      break;   \n","env.close()\n","show_video(path_prefix=logging_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQxxQn-bTQJO"},"source":["#### Replay buffer"]},{"cell_type":"code","metadata":{"id":"A142NhlFED7s"},"source":["class QReplayBuffer:\n","    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n","    def __init__(self, buffer_size=int(1e5), batch_size=64, seed=42, device='cpu'):\n","        \"\"\"Initialize a ReplayBuffer object.\n","        Params:\n","            buffer_size (int): maximum size of buffer\n","            batch_size (int): size of each training batch\n","            seed (int): random seed\n","            device (str): device where tensors are proecssed\n","        \"\"\"\n","\n","        self.memory = deque(maxlen=buffer_size)\n","        self.batch_size = batch_size\n","        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n","        self.device = device\n","        self.seed = random.seed(seed)\n","\n","    def add(self, state, action, reward, next_state, done):\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        e = self.experience(state, action, reward, next_state, done)\n","        self.memory.append(e)\n","\n","    def sample(self, batch_size=None):\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        batch_size = batch_size if batch_size is not None else self.batch_size\n","        batch = random.sample(self.memory, k=batch_size)\n","\n","        states = torch.from_numpy(np.vstack([b.state for b in batch if b is not None])).float().to(self.device)\n","        actions = torch.from_numpy(np.vstack([b.action for b in batch if b is not None])).long().to(self.device)\n","        rewards = torch.from_numpy(np.vstack([b.reward for b in batch if b is not None])).float().to(self.device)\n","        next_states = torch.from_numpy(np.vstack([b.next_state for b in batch if b is not None])).float().to(self.device)\n","        dones = torch.from_numpy(np.vstack([b.done for b in batch if b is not None]).astype(np.uint8)).float().to(self.device)\n","        return (states, actions, rewards, next_states, dones)\n","\n","    def is_ready_to_sample(self):\n","        return len(self) > self.batch_size\n","\n","    def __len__(self):\n","        \"\"\"Return the current size of internal memory.\"\"\"\n","        return len(self.memory)\n","\n","    def set_device(self, device):\n","        self.device = device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_LTp2J35o-X"},"source":["##########################################################################################\n","# Run through the env multiple times and fill in the buffer. Then sample several batches #\n","##########################################################################################\n","game = \"LunarLander-v2\"\n","env = gym.make(game)\n","replay_buffer = QReplayBuffer(batch_size=30)\n","\n","for _ in tqdm.tqdm(range(100)):\n","  state = env.reset()\n","  while True:\n","      action = env.action_space.sample()      \n","      next_state, reward, done, info = env.step(action)\n","      replay_buffer.add(state, action, reward, next_state, done)\n","      state = next_state\n","      if done: \n","        break;   \n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMHG_ZvP5pEH"},"source":["len(replay_buffer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K27Ql2gL5pJt"},"source":["states, actions, rewards, next_states, dones = replay_buffer.sample(2)\n","states"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2vLWqoveTo58"},"source":["#### Q network"]},{"cell_type":"code","metadata":{"id":"vcBEggxmrDAd"},"source":["class DuelingQNetwork(torch.nn.Module):\n","    def __init__(self, state_size, action_size, seed):\n","        \"\"\"Initialize parameters and build model.\n","        Params:\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","            seed (int): Random seed\n","        \"\"\"\n","        super(DuelingQNetwork, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","\n","        self.bn0 = torch.nn.BatchNorm1d(state_size)\n","        self.fc_layer1 = torch.nn.Linear(state_size, 64)\n","        torch.nn.init.xavier_normal_(self.fc_layer1.weight)\n","        self.fc_bn1 = torch.nn.BatchNorm1d(64)\n","\n","        self.fc_layer2 = torch.nn.Linear(64, 64)\n","        torch.nn.init.xavier_normal_(self.fc_layer2.weight)\n","        self.fc_bn2 = torch.nn.BatchNorm1d(64)\n","\n","        self.value_layer1 = torch.nn.Linear(64, 32)\n","        torch.nn.init.xavier_normal_(self.value_layer1.weight)\n","        self.v_bn1 = torch.nn.BatchNorm1d(32)\n","        self.value_layer2 = torch.nn.Linear(32, 1)\n","\n","        self.action_layer1 = torch.nn.Linear(64, 32)\n","        torch.nn.init.xavier_normal_(self.action_layer1.weight)\n","        self.a_bn1 = torch.nn.BatchNorm1d(32)\n","        self.action_layer2 = torch.nn.Linear(32, action_size)\n","\n","    def forward(self, state):\n","        \"\"\"Build a network that maps state -> action values.\"\"\"\n","        x = state\n","        #x = self.bn0(x)\n","\n","        # Shared part of network.\n","        x = self.fc_layer1(x)\n","        torch.nn.functional.leaky_relu_(x)\n","        #x = self.fc_bn1(x)\n","\n","        x = self.fc_layer2(x)\n","        torch.nn.functional.leaky_relu_(x)\n","        #x = self.fc_bn2(x)\n","\n","        # Value part of network.\n","        v = self.value_layer1(x)\n","        torch.nn.functional.leaky_relu_(v)\n","        #v = self.v_bn1(v)        \n","        v = self.value_layer2(v)\n","\n","        # Advantage part of network\n","        a = self.action_layer1(x)\n","        torch.nn.functional.leaky_relu_(a)\n","        #a = self.a_bn1(a)        \n","        a = self.action_layer2(a)\n","\n","        return v + (a - torch.mean(a, dim=1).unsqueeze(1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFdIBB2H89iS"},"source":["qnetwork = DuelingQNetwork(state_size=8, action_size=4, seed=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u2m5Fw-V9U9W"},"source":["qnetwork.eval()\n","##################################################################\n","# Use replay buffer and let neural net predict over the batches. #\n","##################################################################\n","result = qnetwork(states)\n","states, result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-2WOMfunW53Y"},"source":["#### Agent"]},{"cell_type":"code","metadata":{"id":"vbWCpj3l92Yx"},"source":["class QAgent():\n","    \"\"\"Interacts with and learns from the environment.\"\"\"\n","    def __init__(self, state_size, action_size, tau=1e-3, eps=0., gamma=0.99, lr=5e-4, seed=0, device='cpu'):\n","        \"\"\"Initialize an Agent object.\n","        Params:\n","            state_size (int): Dimension of input state.\n","            action_size (int): Dimension of actions.\n","            seed (int): Random seed for reproducibility.\n","            lr (float): Learning rate.\n","            gamma (float): Reward discount.\n","            tau (float): For soft update of target network parameters (tau is weight for local network).\n","            eps (float): For epsilon-greedy action selection, higher eps means more exploration.\n","        \"\"\"\n","        self.device = device\n","        self.gamma = gamma\n","        self.tau = tau\n","        self.eps = eps\n","        self.state_size = state_size\n","        self.action_size = action_size\n","\n","        self.qnetwork_target = DuelingQNetwork(state_size, action_size, seed).to(device)\n","        self.qnetwork_local = DuelingQNetwork(state_size, action_size, seed).to(device)\n","        self.optimizer = torch.optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n","\n","    def __repr__(self):\n","        return f'QAgent(state_size={self.state_size}, action_size={self.action_size}, device=\"{self.device}\")'\n","\n","    def act(self, state, eps=None):\n","        \"\"\"Return action for given state.\n","        Params:\n","            state (numpy.array): Current state.\n","            eps (float): Eps overlad.\n","        \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n","        if eps is None:\n","            eps = self.eps\n","\n","        is_training = self.qnetwork_local.training\n","        self.qnetwork_local.eval()\n","        with torch.no_grad():\n","            action_values = self.qnetwork_local(state)\n","        self.qnetwork_local.train(is_training)\n","\n","        # Epsilon-greedy action selection\n","        if random.random() > eps:\n","            return np.argmax(action_values.cpu().data.numpy())\n","        else:\n","            return random.choice(np.arange(self.action_size))\n","\n","    def train(self, replay_buffer, gamma=None, tau=None, batch_size=None):\n","        \"\"\"Update value parameters using sampled batches from replay buffer.\n","        Params:\n","            replay_buffer: Buffer with records from history.\n","            gamma (float): Discount factor.\n","            tau (float): For soft update of target network parameters\n","        \"\"\"\n","\n","        if gamma is None:\n","            gamma = self.gamma\n","        if not replay_buffer.is_ready_to_sample():\n","            return None\n","        batch = replay_buffer.sample(batch_size)\n","        states, actions, rewards, next_states, dones = batch\n","        \n","        self.qnetwork_local.train()\n","        target = rewards + gamma * self.qnetwork_target(next_states).max(dim=1)[0].unsqueeze(1) * (1 - dones)\n","        prediction = self.qnetwork_local(states).gather(dim=1, index=actions)\n","        loss = torch.nn.functional.mse_loss(target, prediction)\n","        \n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n","\n","    def soft_update(self, local_model, target_model, tau=None):\n","        \"\"\"Soft update model parameters.\n","        θ_target = τ*θ_local + (1 - τ)*θ_target\n","        Params:\n","            local_model (PyTorch model): weights will be copied from\n","            target_model (PyTorch model): weights will be copied to\n","            tau (float): For soft update of target network parameters\n","        \"\"\"\n","        if tau is None:\n","            tau = self.tau\n","\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n","\n","    def set_device(self, device):\n","      self.device = device\n","      self.qnetwork_local = self.qnetwork_local.to(device)\n","      self.qnetwork_target = self.qnetwork_target.to(device)\n","    \n","    def save(self, path):\n","        folder, file = path.rsplit('/', 1)\n","        if not os.path.exists(folder):\n","            os.makedirs(folder)\n","        torch.save(self.qnetwork_local.state_dict(), path)\n","\n","    def load(self, path):\n","        self.qnetwork_local.load_state_dict(torch.load(path))\n","        self.qnetwork_target.load_state_dict(torch.load(path))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLjIhRAkRqbz"},"source":["agent = QAgent(state_size=8, action_size=4, seed=0)\n","agent.act(state=np.array([1, 2, 3, 4, 5, 6, 7, 8]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t2YqeC17ZaPG"},"source":["#### Q learning algorithm"]},{"cell_type":"code","metadata":{"id":"QpUyxEWDRXz1"},"source":["#######################\n","# Setup of parameters #\n","#######################\n","\n","episodes = 2000                           # Number of episodes played.\n","steps_per_episodes = 1000                 # Maximal amount of steps in one episode.\n","batch_size = 64                           # Size of batches sampled during training from replay buffer.\n","train_rate = 4                            # Rate of episodes including training (each train_rate th).\n","eps = 1.0                                 # Eps params cotroling exploration / exploitation.\n","eps_decay = 0.995\n","eps_min = 0.01\n","gamma = 0.99                              # Reward discounting.\n","stop_reward = 240                         # Average reward from 100 consecutive runs which would stop algorithm.\n","rewards_window = deque(maxlen=100)        # Buffer for 100 consecutive run rewards.        \n","rewards = []                              # Log of all episode rewards.\n","model_name = 'luna_q'                     # Identifier of model saved params.\n","game = \"LunarLander-v2\"\n","seed=42\n","env = gym.make(game)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","agent = QAgent(state_size=8, action_size=4, seed=0, device=device)\n","replay_buffer = QReplayBuffer(buffer_size=int(1e5), \n","                             batch_size=batch_size, \n","                             seed=seed, device=device)\n","\n","\n","#######################\n","# Q lerning algorithm #\n","#######################\n","\n","for episode_id in range(episodes):\n","    episode_reward = 0\n","    # At the start of episode, we restart the environment.\n","    state = env.reset()\n","    # Here starts episode.\n","    for t_step in range(steps_per_episodes):\n","        # Agent selects action based on current policy.\n","        action = agent.act(state, eps)\n","        next_state, reward, done, info = env.step(action)\n","        # Save experience into replay buffer.\n","        replay_buffer.add(state, action, reward, next_state, done)\n","        # Train with train_rate.\n","        if t_step % train_rate == 0:\n","            agent.train(replay_buffer, gamma=gamma, batch_size=batch_size)          \n","        episode_reward += reward\n","        if done:\n","            break\n","        state = next_state\n","    # Afet each episode we exploit current policy more.\n","    eps = max(eps*eps_decay, eps_min)\n","\n","    # Reporting.\n","    rewards_window.append(episode_reward)\n","    rewards.append(episode_reward)\n","    print(f'\\rEpisode {episode_id}\\tAverage Score: {np.round(np.mean(rewards_window), 1)}', end=\"\")\n","    if episode_id % 100 == 0:\n","        print(f'\\rEpisode {episode_id}\\tAverage Score: {np.round(np.mean(rewards_window), 1)}')\n","        agent.save(path=os.path.join(rl_workshop_path, f'models/{model_name}-{episode_id}-{np.round(np.mean(rewards_window), 1)}-{np.round(episode_reward, 1)}.pth'))\n","    if np.mean(rewards_window)>=stop_reward:\n","        print(f'\\nSolved! Took {episode_id-100} episodes\\tAverage Score: {np.round(np.mean(rewards_window), 1)}')\n","        agent.save(path=os.path.join(rl_workshop_path, f'models/{model_name}-{episode_id}-{np.round(np.mean(rewards_window), 1)}-{np.round(episode_reward, 1)}.pth'))\n","        break\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m98C4oHLcoPO"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G79m7wnHgfnF"},"source":["fig = plt.figure(figsize=(10, 10))\n","plt.plot(np.arange(len(rewards)), rewards)\n","plt.ylabel('Reward')\n","plt.xlabel('Episode')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaJBFA78Palz"},"source":["!ls '/content/drive/My Drive/ml_college_data/rl_workshop/models/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbYMxpxScrb2"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","agent = QAgent(state_size=8, action_size=4, seed=0, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmjSEvUuicP_"},"source":["model_path = os.path.join(rl_workshop_path, 'models/luna-657-240.3-274.9.pth')\n","video_path = '/video/lunar/'\n","game = \"LunarLander-v2\"\n","\n","agent.load(path=model_path)\n","env = wrap_env(gym.make(game), video_path)\n","state = env.reset()\n","while True:\n","    env.render('human')\n","    action = agent.act(state, eps=0)      \n","    state, reward, done, info = env.step(action) \n","    if done: \n","      break;   \n","env.close()\n","show_video(video_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nz61CWqhRDL"},"source":[" #### Questions and experiment suggestions:\n"," - Try benefit of weighted replay buffer (weighting by current error)\n"," - Simplified vs batch normed vs dueling network \n"," - Different eps values during training and evaluation - what are the differences?\n"," - Try different environments"]},{"cell_type":"markdown","metadata":{"id":"VMPu8R25ArTd"},"source":["## Policy Based Methods - Reinforce\n"," - cart pole environment \n"," - simple policy network with categorical sampling\n"," - building of reinforce loss\n"," - reinfroce agent\n"," - reinforce algorith "]},{"cell_type":"markdown","metadata":{"id":"JukkY6-xky5L"},"source":["#### CartPole environment"]},{"cell_type":"code","metadata":{"id":"q530JSUUk7Z4"},"source":["game = \"CartPole-v0\"\n","env = gym.make(game)\n","\n","###########################\n","# Explore the environment #\n","###########################\n","\n","\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N44AsCp6Lump"},"source":["#### Policy network"]},{"cell_type":"code","metadata":{"id":"y4rcSy6xmAzn"},"source":["class ReinforcePolicyNetwork(torch.nn.Module):\n","    def __init__(self, state_size, action_size, seed):\n","        \"\"\"Initialize parameters and build model.\n","        Params:\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","            seed (int): Random seed\n","        \"\"\"\n","        super(ReinforcePolicyNetwork, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","\n","        self.fc_layer1 = torch.nn.Linear(state_size, 64)\n","        torch.nn.init.xavier_normal_(self.fc_layer1.weight)\n","\n","        self.fc_layer2 = torch.nn.Linear(64, 32)\n","        torch.nn.init.xavier_normal_(self.fc_layer2.weight)\n","\n","        self.action_layer = torch.nn.Linear(32, action_size)\n","\n","    def forward(self, state):\n","        \"\"\"Build a network that maps state -> action values.\"\"\"\n","        x = state\n","        x = self.fc_layer1(x)\n","        torch.nn.functional.leaky_relu_(x)\n","\n","        x = self.fc_layer2(x)\n","        torch.nn.functional.leaky_relu_(x)\n","     \n","        logits = self.action_layer(x)\n","        probs =  torch.nn.functional.softmax(logits, dim=1)\n","        # We return both raw values and probabilities.\n","        return logits, probs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WDt1aWAjndtV"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","policy = ReinforcePolicyNetwork(state_size=8, action_size=4, seed=42)\n","policy = policy.to(device)\n","policy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1aCw6LGBn7n6"},"source":["input = torch.tensor([[1, 2, 3, 4, 5, 61, 71, 81]], device=device, dtype=torch.float32)\n","logits, probs = policy(input)\n","probs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4s4okIYMUkL"},"source":["##################################################################\n","# Use Categorial to sample actions from probability distribution #\n","#                        given by probs.                         #\n","##################################################################\n","\n","dist = Categorical(probs)\n","actions = dist.sample()\n","#actions = torch.argmax(probs, dim=1)\n","actions, actions.cpu().item()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dl5HUVeOxZB7"},"source":["#### Construct Reinforce Loss Function"]},{"cell_type":"code","metadata":{"id":"dji3pYgCxbw9"},"source":["def get_online_reinforce_loss(action_probs, rewards, gamma=1):\n","    ##############################################################################\n","    #       Build discounted reward for each action of one episode               #\n","    # and calculate loss. Don't forget that we are minimizing the objectiv now.  #\n","    ##############################################################################\n","\n","    loss = []\n","    discounts = [gamma**i for i in range(len(rewards)+1)]\n","    R = sum([a*b for a,b in zip(discounts, rewards)])\n","    discounts = np.array(discounts)\n","    rewards = np.array(rewards)\n","    \n","    for i, prob in enumerate(action_probs):\n","        loss.append((-torch.log(prob) * sum(rewards[i:]*discounts[:-(1+i)])).unsqueeze(0))\n","        #loss.append(-torch.log(prob).unsqueeze(0) * R)\n","    return torch.cat(loss).sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9XrHJmAy3f1"},"source":["action_probs = torch.tensor([0.3, 0.1, 0.5], requires_grad=True)\n","rewards = [1,-1 , 1]\n","\n","loss = get_online_reinforce_loss(action_probs, rewards)\n","print(loss)\n","assert loss.requires_grad == True\n","assert loss.cpu().item() in [1.8971199989318848, 4.199705123901367]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBvSeOIuNWGc"},"source":["#### Reinforce Agent"]},{"cell_type":"code","metadata":{"id":"54KAH90jAupN"},"source":["class ReinforceAgent():\n","    \"\"\"Interacts with and learns from the environment.\"\"\"\n","    def __init__(self, state_size, action_size, gamma=1, lr=1e-3, seed=42, device='cpu'):\n","        \"\"\"Initialize an Agent object.\n","        Params:\n","            state_size (int): Dimension of input state.\n","            action_size (int): Dimension of actions.\n","            seed (int): Random seed for reproducibility.\n","            gamma (float): Reward discount.\n","        \"\"\"\n","        self.device = device\n","        self.gamma = gamma\n","        self.state_size = state_size\n","        self.action_size = action_size\n","\n","        self.policy = ReinforcePolicyNetwork(state_size, action_size, seed).to(device)\n","        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n","\n","    def __repr__(self):\n","        return f'ReinforceAgent(state_size={self.state_size}, action_size={self.action_size}, device=\"{self.device}\")'\n","\n","    def act(self, state, training_process=True, deterministic=False):\n","        \"\"\"Return action and it's probability (with requires_grad) for given state and current policy.\n","        Params:\n","            state (numpy.array): Current state.\n","            training_process (bool): Marks whether we keep info for gradient descent or not.\n","            deterministic (bool): If False, we sample action from distribution, otherwise we take maximum.\n","        \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n","        \n","        is_training = self.policy.training\n","        if training_process:\n","            self.policy.train()\n","            logits, probs = self.policy(state)\n","        else:\n","            self.policy.eval()\n","            with torch.no_grad():\n","                logits, probs = self.policy(state)    \n","        self.policy.train(is_training)\n","\n","        ##########################################################################\n","        # Sample action from distribution if deterministic=False, else take max  #\n","        #              and return -> action id, action probability scalar        #\n","        ##########################################################################\n","        #return 0, probs[0,0]\n","        dist = Categorical(probs)\n","        if deterministic:\n","          action = torch.argmax(probs, dim=1)\n","        else:\n","          action = dist.sample()\n","        action = action.cpu().item()\n","        return action, probs[0, action]\n","\n","\n","    def train(self, action_probs, rewards, gamma=None):\n","        \"\"\"Update policy parameters using action probs and rewards from given episode.\n","        Params:\n","            rewards (list): List of rewards from the whole episode.\n","            action_probs (torch.tensor): Probabilities of sampled actions\n","            gamma (float): discount factor\n","\n","        \"\"\"\n","        if gamma is None:\n","            gamma = self.gamma\n","        loss = get_online_reinforce_loss(action_probs=action_probs, rewards=rewards, gamma=gamma)\n","        \n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def set_device(self, device):\n","      self.device = device\n","      self.policy = self.policy.to(device)\n","    \n","    def save(self, path):\n","        folder, file = path.rsplit('/', 1)\n","        if not os.path.exists(folder):\n","            os.makedirs(folder)\n","        torch.save(self.policy.state_dict(), path)\n","\n","    def load(self, path):\n","        self.policy.load_state_dict(torch.load(path))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AG0RPdVMp5e8"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","reinforce_agent = ReinforceAgent(4, 2, device=device)\n","reinforce_agent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c26IQshKOQ2r"},"source":["reinforce_agent.act(state=np.array([1, 2, 3, 4]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vRzZJsv4YJc9"},"source":["#### Reinforce learning"]},{"cell_type":"code","metadata":{"id":"JiLllvrhEE2s"},"source":["#######################\n","# Setup of parameters #\n","#######################\n","\n","episodes = 20000\n","steps_per_episodes = 1000\n","stop_reward = 200\n","rewards_window = deque(maxlen=100)\n","rewards = []\n","gamma = 1\n","lr = 0.001\n","model_name='pole'\n","game = \"CartPole-v0\"\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","env = gym.make(game)\n","agent = ReinforceAgent(state_size=4, action_size=2, seed=0, gamma=gamma, lr=lr, device=device)\n","\n","#######################\n","# Reinforce algorithm #\n","#######################\n","\n","for episode_id in range(episodes):\n","    # List of rewards collected during one episode.\n","    episode_rewards = []\n","    # Action probs collected during one episode.\n","    episode_action_probs = []\n","    state = env.reset()\n","    for _ in range(steps_per_episodes):\n","        # Keep information about gradient flow.\n","        action, action_prob = agent.act(state, training_process=True, deterministic=False)\n","        next_state, reward, done, info = env.step(action)\n","        # Simplified buffer fill in.\n","        episode_rewards.append(reward)\n","        episode_action_probs.append(action_prob)\n","        if done:\n","            break\n","        state = next_state\n","    # Agent is trained after each episode.\n","    agent.train(action_probs=episode_action_probs, rewards=episode_rewards)\n","\n","    # Reporting.\n","    episode_reward = sum(episode_rewards)\n","    rewards_window.append(episode_reward)\n","    rewards.append(episode_reward)\n","    print(f'\\rEpisode {episode_id}\\tAverage Score: {np.round(np.mean(rewards_window), 1)}', end=\"\")\n","    if episode_id % 100 == 0:\n","        print(f'\\rEpisode {episode_id}\\tAverage Score: {np.round(np.mean(rewards_window), 1)}')\n","        agent.save(path=os.path.join(rl_workshop_path, f'models/{model_name}-{episode_id}-{np.round(np.mean(rewards_window), 1)}-{np.round(episode_reward, 1)}.pth'))\n","    if np.mean(rewards_window)>=stop_reward:\n","        print(f'\\nSolved! Took {episode_id-100} episodes\\tAverage Score: {np.round(np.mean(rewards_window), 1)}')\n","        agent.save(path=os.path.join(rl_workshop_path, f'models/{model_name}-{episode_id}-{np.round(np.mean(rewards_window), 1)}-{np.round(episode_reward, 1)}.pth'))\n","        break\n","\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qb-1IE46SDYJ"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Hpdcez_bD3A"},"source":["fig = plt.figure(figsize=(10, 10))\n","plt.plot(np.arange(len(rewards)), rewards)\n","plt.ylabel('Reward')\n","plt.xlabel('Episode')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rOIS3hlPHQzP"},"source":["!ls '/content/drive/My Drive/ml_college_data/rl_workshop/models/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZmdTmq2dOWeA"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","agent = ReinforceAgent(4, 2, device=device)\n","agent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MORQnBJZ4k3H"},"source":["model_path = os.path.join(rl_workshop_path, 'models/pole-815-200.0-200.0.pth')\n","video_path = '/video/pole_exp1/'\n","game = \"CartPole-v0\"\n","\n","agent.load(path=model_path)\n","env = wrap_env(gym.make(game), video_path)\n","state = env.reset()\n","while True:\n","    env.render('human')\n","    action, action_prob = agent.act(state, training_process=False, deterministic=True)      \n","    state, reward, done, info = env.step(action) \n","    if done: \n","      break;   \n","env.close()\n","show_video(video_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JPQVpjTidM6q"},"source":["#### Questions and experiment suggestions\n"," - Use reward just from future for each action\n"," - What would happen if we would normalize reward\n"," - Train other environments, e.g. `LunarLander` again.."]},{"cell_type":"markdown","metadata":{"id":"ccCHYqnR4cMY"},"source":["## Policy based methods - PPO\n"," - pong environment\n"," - preparing input data for agent\n"," - computing PPO loss function"]},{"cell_type":"markdown","metadata":{"id":"lvFSwWbsdedX"},"source":["#### Observe Pong Env"]},{"cell_type":"code","metadata":{"id":"ntrzMLAveDVw"},"source":["game = \"Pong-v0\"\n","env = gym.make(game)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qN0lHAcZejMQ"},"source":["env.observation_space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rr0RHoz1A_kh"},"source":["env.action_space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9ZxABy5dk2i"},"source":["env.get_action_meanings()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBpLQWLZiwSV"},"source":["state = env.reset()\n","state.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2Hu4GBKi59e"},"source":["plt.figure(figsize=(7, 7))\n","plt.imshow(state)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKokHbMutgn6"},"source":["next_state, reward, done, info = env.step(1)\n","reward, done, info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5-KCCWC7BCeQ"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ezgWQ74xeLdh"},"source":["#### State space preprocessing"]},{"cell_type":"code","metadata":{"id":"mP6XaJueBzF_"},"source":["def state_preprocess(state):\n","    \"\"\" Preprocess 210x160x3 uint8 frame into 6000 (75x80) 1D float vector. \"\"\"\n","    #############################################################################\n","    #  Remove from the state as many pixels as possible and keep important info #\n","    #   - dowscale                                                              #\n","    #   - greyscale                                                             #\n","    #   - cutoffs                                                               #\n","    #############################################################################\n","\n","    \"\"\"\n","      if state is None:\n","          return torch.zeros(210*160*3)\n","      return state.astype(np.float32).ravel()\n","    \"\"\"\n","\n","    if state is None:\n","        return torch.zeros(6000)\n","\n","    state = state[35:185]     # Crop - remove 35px from start & 25px from end of image in x, to reduce redundant parts of image (i.e. after ball passes paddle).\n","    state = state[::2,::2,0]  # Downsample by factor of 2.\n","    state[state == 144] = 0   # Erase background (background type 1).\n","    state[state == 109] = 0   # Erase background (background type 2)\n","    state[state != 0] = 1     # Everything else (paddles, ball) just set to 1. this makes the image grayscale effectively\n","    return state.astype(np.float32).ravel()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLY-ilPETmq5"},"source":["def get_final_state(state, prev_state):\n","    #######################################################\n","    # Experiment with combination of 2 successing images  #\n","    #######################################################\n","    #return state_preprocess(state) - state_preprocess(prev_state)\n","    \n","    return np.concatenate([state_preprocess(state), state_preprocess(prev_state)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QiqS7dqQE74"},"source":["state_preprocess(None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PymJLkUjLkF"},"source":["state.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5dbrsnRLjCgJ"},"source":["plt.figure(figsize=(7,7))\n","plt.imshow(state_preprocess(state).reshape((75, 80)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ASC1X9GUI83"},"source":["get_final_state(state, None).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_B7s8AaeeWAj"},"source":["#### Replay Buffer"]},{"cell_type":"code","metadata":{"id":"QmB-Xs1R-fS1"},"source":["def get_discounted_rewards(rewards, gamma):\n","    discounted_rewards = []\n","    dr = 0\n","    for r in rewards[::-1]:\n","        # Specific for pong - we can recognize episodes.\n","        if r != 0: \n","          dr = 0\n","        dr = r + gamma * dr\n","        discounted_rewards.insert(0, dr)\n","\n","    discounted_rewards = np.array(discounted_rewards)\n","    std = discounted_rewards.std()\n","    std = 1 if std==0 else std\n","    return (discounted_rewards - discounted_rewards.mean()) / std"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgUZlXR3_DG7"},"source":["get_discounted_rewards(np.array([0,0,0,0,1,0,0,0,0,-1]), gamma=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5O1WBx0f1ZK"},"source":["class PPOReplayBuffer:\n","    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n","    def __init__(self, batch_size=64, gamma=0.99, seed=42, device='cpu'):\n","        \"\"\"Initialize a ReplayBuffer object.\n","        Params:\n","            batch_size (int): size of each training batch\n","            seed (int): random seed\n","            device (str): device where tensors are proecssed\n","        \"\"\"\n","        self.batch_size = batch_size\n","        self.gamma = gamma\n","        self.is_reward_recalculated = False\n","        self.device = device\n","        self.seed = random.seed(seed)\n","\n","        # Those are values we want to collect\n","        self.states = []\n","        self.actions = []\n","        self.action_probs = []\n","        self.rewards = []\n","        self.discounted_rewards = []\n","\n","    def add(self, state, action, action_prob, reward):\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        self.is_reward_recalculated = False\n","        self.states.append(state)\n","        self.actions.append(action)\n","        self.action_probs.append(action_prob)\n","        self.rewards.append(reward)\n","\n","    def sample(self, batch_size=None):\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        idxs = random.sample(range(len(self.actions)), batch_size)\n","\n","        # We need to recalculate because of normalization.\n","        if  not self.is_reward_recalculated:\n","          self.update_discounted_rewards()\n","\n","        states_batch = torch.from_numpy(np.vstack([self.states[idx] for idx in idxs])).float().to(self.device)\n","        actions_batch = torch.from_numpy(np.vstack([self.actions[idx] for idx in idxs])).long().to(self.device).squeeze()\n","        action_probs_batch = torch.from_numpy(np.vstack([self.action_probs[idx] for idx in idxs])).float().to(self.device).squeeze()\n","        rewards_batch = torch.from_numpy(np.vstack([self.discounted_rewards[idx] for idx in idxs])).float().to(self.device).squeeze()\n","        return states_batch, actions_batch, action_probs_batch, rewards_batch\n","\n","    def update_discounted_rewards(self, gamma=None):\n","      gamma = self.gamma if gamma is None else gamma\n","      self.discounted_rewards = get_discounted_rewards(self.rewards, gamma)\n","      self.is_reward_recalculated = True\n","\n","    def is_ready_to_sample(self):\n","        return len(self) > self.batch_size\n","\n","    def empty_buffer(self):\n","        # Because of big variance, we clean up buffer frequently.\n","        self.states = []\n","        self.actions = []\n","        self.action_probs = []\n","        self.rewards = []\n","        self.discounted_rewards = []\n","\n","    def __len__(self):\n","        \"\"\"Return the current size of internal memory.\"\"\"\n","        return len(self.actions)\n","\n","    def set_device(self, device):\n","        self.device = device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16BhsTiSf6qD"},"source":["buffer = PPOReplayBuffer()\n","for i in range(10):\n","  buffer.add([1, 2, 3, 4], 1, 0.74, 1 if i%4==0 and i>0 else 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7UPCl3OgUNx"},"source":["state, action, action_prob, reward = buffer.sample(4)\n","reward, action_prob"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5GfRHKWe6LO"},"source":["#### Policy network"]},{"cell_type":"code","metadata":{"id":"jsrzzYjgg_EK"},"source":["class PPOPolicyNetwork(torch.nn.Module):\n","    def __init__(self, state_size, action_size, seed):\n","        \"\"\"Initialize parameters and build model.\n","        Params:\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","            seed (int): Random seed\n","        \"\"\"\n","        super(PPOPolicyNetwork, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","\n","        self.fc_layer1 = torch.nn.Linear(state_size, 512)\n","        torch.nn.init.xavier_normal_(self.fc_layer1.weight)\n","\n","        #self.fc_layer2 = torch.nn.Linear(64, 32)\n","        #torch.nn.init.xavier_normal_(self.fc_layer2.weight)\n","\n","        self.action_layer = torch.nn.Linear(512, action_size)\n","\n","    def forward(self, state):\n","        \"\"\"Build a network that maps state -> action values.\"\"\"\n","        x = state\n","        x = self.fc_layer1(x)\n","        torch.nn.functional.leaky_relu_(x)\n","\n","        #x = self.fc_layer2(x)\n","        #torch.nn.functional.leaky_relu_(x)\n","     \n","        logits = self.action_layer(x)\n","        probs =  torch.nn.functional.softmax(logits, dim=1)\n","        # Return both logits and probs.\n","        return logits, probs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gWckoo2gYDo5"},"source":["states_batch, actions_batch, action_probs_batch, rewards_batch = buffer.sample(4)\n","states_batch, actions_batch, action_probs_batch, rewards_batch "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iXvEVRhvumZm"},"source":["policy = PPOPolicyNetwork(state_size=4, action_size=2, seed=42)\n","policy(states_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WeTeQOFTfcvg"},"source":["#### Constructing PPO loss functions"]},{"cell_type":"code","metadata":{"id":"G9ZRt5_UgBC2"},"source":["states_batch = torch.tensor([[1., 2., 3., 4.],\n","                            [1., 2., 3., 4.],\n","                            [1., 2., 3., 4.],\n","                            [1., 2., 3., 4.]])\n","\n","actions_batch = torch.tensor([1, 1, 1, 1])\n","action_probs_batch = torch.tensor([0.7400, 0.7400, 0.7400, 0.7400])\n","rewards_batch = torch.tensor([0.3778, 0.2776, 0.3106, 0.3778])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oURozEgjlJD6"},"source":["def get_new_action_probs(policy, states, actions, device='cpu'):\n","    \"\"\" Return new probabilities for  actions. Works only for 2 action state space! \"\"\"\n","    # Selector can be generalized using identity matrix for more actions.\n","\n","    ###################################\n","    # Analyze what code actually does #\n","    ###################################\n","    selector = np.array([[1., 0.], [0., 1.]])\n","    selector = torch.FloatTensor(selector[actions.cpu().numpy()]).squeeze(1).to(device)\n","\n","    logits, probs = policy(states)\n","    action_probs = torch.sum(probs * selector, dim=1) \n","    return action_probs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGH0fW23v6U4"},"source":["get_new_action_probs(policy, states_batch, actions_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TDpIwTjWo8M_"},"source":["def get_ppo_loss(policy, states, actions, action_probs, rewards, eps_clip=0.1, device='cpu'):\n","    new_action_probs = get_new_action_probs(policy, states, actions, device=device)\n","\n","    r = new_action_probs / action_probs\n","    loss1 = r * rewards\n","    loss2 = torch.clamp(r, 1-eps_clip, 1+eps_clip) * rewards\n","    loss = -torch.min(loss1, loss2)\n","    loss = torch.mean(loss)\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYC1pqZAmxjr"},"source":["assert get_ppo_loss(policy, states_batch, actions_batch, action_probs_batch, rewards_batch, eps_clip=0.1).item() == -0.22267144918441772"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-zg8juLmxs3"},"source":["def get_reinforce_loss(policy, states, actions, action_probs, rewards):\n","    logits, probs = policy(states)\n","    loss = torch.nn.functional.cross_entropy(logits, actions, reduction='none') * rewards\n","    loss = torch.mean(loss)\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YnvUlQzRxLZs"},"source":["assert get_reinforce_loss(policy, states_batch, actions_batch, action_probs_batch, rewards_batch).item() == 0.23932071030139923"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R2Yl7MffgIEN"},"source":["#### PPO Agent"]},{"cell_type":"code","metadata":{"id":"dfZ7UY2jJxpC"},"source":["class PPOAgent():\n","    \"\"\"Interacts with and learns from the environment.\"\"\"\n","    def __init__(self, state_size, action_size, lr=1e-3, seed=42, device='cpu'):\n","        \"\"\"Initialize an Agent object.\n","        Params:\n","            state_size (int): Dimension of input state.\n","            action_size (int): Dimension of actions.\n","            seed (int): Random seed for reproducibility.\n","            gamma (float): Reward discount.\n","        \"\"\"\n","        self.device = device\n","        self.state_size = state_size\n","        self.action_size = action_size\n","\n","        self.policy = PPOPolicyNetwork(state_size, action_size, seed).to(device)\n","        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n","\n","    def __repr__(self):\n","        return f'PPOAgent(state_size={self.state_size}, action_size={self.action_size}, device=\"{self.device}\")'\n","\n","    def act(self, state, deterministic=False):\n","        \"\"\"Return actions and probabilities for given state and current policy.\n","        Params:\n","            state (array_like): Current state.\n","            deterministic (bool): If False, we sample action from distribution, otherwise we take maximum.\n","        ReturnL \n","        \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n","        is_training = self.policy.training\n","\n","        self.policy.eval()\n","        with torch.no_grad():\n","            logits, probs = self.policy(state)\n","        dist = Categorical(probs)\n","        if deterministic:\n","            actions = torch.argmax(probs, dim=1)    \n","        else:\n","            actions = dist.sample()\n","        self.policy.train(is_training)\n","        return actions.cpu().item(), probs[0, actions].cpu().item()\n","\n","    def train(self, buffer, training_iterations, batch_size=64, eps_clip=0.1):\n","        for _ in range(training_iterations):\n","          states_batch, actions_batch, action_probs_batch, rewards_batch = buffer.sample(batch_size)     \n","          self.optimizer.zero_grad()\n","          loss = get_ppo_loss(self.policy, states=states_batch, \n","                              actions=actions_batch, \n","                              action_probs=action_probs_batch, \n","                              rewards=rewards_batch, \n","                              eps_clip=eps_clip, \n","                              device=self.device)\n","\n","          loss.backward()\n","          self.optimizer.step()\n","\n","    def set_device(self, device):\n","      self.device = device\n","      self.policy = self.policy.to(device)\n","    \n","    def save(self, path):\n","        folder, file = path.rsplit('/', 1)\n","        if not os.path.exists(folder):\n","            os.makedirs(folder)\n","        torch.save(self.policy.state_dict(), path)\n","\n","    def load(self, path):\n","        self.policy.load_state_dict(torch.load(path, map_location=self.device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9eTUkQPW9iuG"},"source":["agent = PPOAgent(action_size=2, state_size=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ot2loJM_ZC0W"},"source":["agent.act(np.array([1,2,3,4]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5225GpdlkMul"},"source":["#### PPO training"]},{"cell_type":"code","metadata":{"id":"HQ5b8Q6bQ8X4"},"source":["#######################\n","# Setup of parameters #\n","#######################\n","\n","game = \"Pong-v0\"\n","episodes = 20000\n","steps_per_episodes = 100000\n","stop_reward = 200\n","gamma = 0.99\n","eps_clip = 0.1\n","model_name='pong'\n","env = gym.make(game)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","agent = PPOAgent(state_size=12000, action_size=2, device=device)\n","replay_buffer = PPOReplayBuffer(gamma=gamma, device=device) \n","rewards_window = deque(maxlen=100)\n","rewards = []\n","\n","\n","###################\n","#  PPO algorithm  #\n","###################\n","\n","for episode_id in range(episodes):\n","    episode_reward = 0\n","    # We need to keep 2 state sequence to get dynamics.\n","    state, prev_state = env.reset(), None\n","    for _ in range(steps_per_episodes):\n","        # Preprocess states.\n","        final_state = get_final_state(state, prev_state)\n","        # Run the agent and get action + prob.\n","        action, action_prob = agent.act(final_state)\n","        # We increase action +2 to keep real semantic.\n","        next_state, reward, done, info = env.step(action+2)\n","        # Save data to replay buffer.\n","        replay_buffer.add(final_state, action, action_prob, reward)\n","        episode_reward += reward\n","        if done:\n","            break\n","        prev_state = state\n","        state = next_state\n","    # Agent is trained after a few episodes & buffer is emptied.\n","    if episode_id % 10:\n","      agent.train(replay_buffer, training_iterations=10, batch_size=512, eps_clip=eps_clip)\n","      replay_buffer.empty_buffer()\n","\n","    # Reporting.\n","    rewards_window.append(episode_reward)\n","    rewards.append(episode_reward)\n","    print(f'\\rEpisode {episode_id}\\tAverage Score: {np.round(np.mean(rewards_window), 1)}', end=\"\")\n","    if episode_id % 50 == 0:\n","        print(f'\\rEpisode {episode_id}\\tAverage Score: {np.round(np.mean(rewards_window), 1)}')\n","        agent.save(path=os.path.join(rl_workshop_path, f'models/{model_name}-{episode_id}-{np.round(np.mean(rewards_window), 1)}-{np.round(episode_reward, 1)}.pth'))\n","    if np.mean(rewards_window)>=stop_reward:\n","        print(f'\\nSolved! Took {episode_id-100} episodes\\tAverage Score: {np.round(np.mean(rewards_window), 1)}')\n","        agent.save(path=os.path.join(rl_workshop_path, f'models/{model_name}-{episode_id}-{np.round(np.mean(rewards_window), 1)}-{np.round(episode_reward, 1)}.pth'))\n","        break\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjGEe8oxkVFD"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AvfmZX8_HOBj"},"source":["!ls '/content/drive/My Drive/ml_college_data/rl_workshop/models/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AK3BlVO-wEOl"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","agent = PPOAgent(state_size=12000, action_size=2, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qF0UsPE-eqfs"},"source":["model_path = os.path.join(rl_workshop_path, 'models/pong-2650--8.1-3.0.pth')\n","agent.load(path=model_path)\n","\n","video_path = '/video/pong_reinforce/'\n","game = \"Pong-v0\"\n","env = wrap_env(gym.make(game), video_path)\n","\n","state, prev_state = env.reset(), None\n","for _ in range(100000):\n","    env.render('human')\n","    # Preprocess state.\n","    final_state = get_final_state(state, prev_state)\n","    # Run the action.\n","    action, action_prob = agent.act(final_state, deterministic=True)\n","    next_state, reward, done, info = env.step(action+2)\n","    if done:\n","        break\n","    prev_state = state\n","    state = next_state \n","env.close()\n","show_video(video_path)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"frwVZbloCDQu"},"source":["#### Questions and experiment suggestions\n"," - Experiment with policy network complexity - adding/removing layers can make big difference.\n"," - Compare the progression with different hyperparam setup.\n"," - Try to use crossentropy loss instead of PPO reweight clipped loss. \n"," - Implement PPO for lunar module - think through discount mechanism."]},{"cell_type":"markdown","metadata":{"id":"7N1Hn0-n-uVv"},"source":["## Actor Critic Methods\n"," - explore motion based environment\n"," - gaussian process for randomizing actions\n"," - actor/critic network\n"," - AC agent\n"," - AC training  "]},{"cell_type":"markdown","metadata":{"id":"ybV1tZdjFX_7"},"source":["#### Environment"]},{"cell_type":"code","metadata":{"id":"8q4k6XxEbhPr"},"source":["env = gym.make('BipedalWalker-v3')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fBEL_f3KR6J"},"source":["env.action_space.sample()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQ8YzV00KSA8"},"source":["env.observation_space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETTtMlG9P6Ir"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0XQ3jQrlKSDO"},"source":["logging_path = '/video/walker/test/'\n","env = wrap_env(gym.make('BipedalWalker-v3'), logging_path)\n","state = env.reset()\n","while True:\n","    # 'human' type of rendenring is suitable when we do the recording\n","    env.render('human')\n","    action = env.action_space.sample()      \n","    state, reward, done, info = env.step(action) \n","    if done: \n","      break;   \n","env.close()\n","show_video(path_prefix=logging_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0dfyX0a-e5p"},"source":["#### Exploration of continuous space"]},{"cell_type":"code","metadata":{"id":"WjMt0kbrkxuo"},"source":["class OUNoise:\n","    \"\"\"Ornstein-Uhlenbeck process\"\"\"\n","\n","    def __init__(self, size, seed, mu=0.0, theta=0.15, sigma=0.2, sigma_min=0.05, sigma_decay=.975):\n","        \"\"\"Initialize parameters and noise process.\"\"\"\n","        self.mu = mu * np.ones(size)\n","        self.theta = theta\n","        self.sigma = sigma\n","        self.sigma_min = sigma_min\n","        self.sigma_decay = sigma_decay\n","        self.seed = random.seed(seed)\n","        self.size = size\n","        self.reset()\n","\n","    def reset(self):\n","        self.state = copy.copy(self.mu)\n","        self.sigma = max(self.sigma_min, self.sigma * self.sigma_decay)\n","\n","    def sample(self):\n","        x = self.state\n","        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n","        self.state = x + dx\n","        return self.state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUtd0F0ZArJS"},"source":["plt.figure(figsize=(7,7))\n","noise = OUNoise(size=1, seed=42)\n","sns.kdeplot([noise.sample()[0] for _ in range(10000)], shade='blue')\n","noise.sigma = 0.05\n","sns.kdeplot([noise.sample()[0] for _ in range(10000)], shade='blue')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJxXO7ChBBMH"},"source":["plt.figure(figsize=(7,7))\n","plt.plot([noise.sample()[0] for _ in range(10000)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gDZE0y91lkUk"},"source":["#### Replay Buffer"]},{"cell_type":"code","metadata":{"id":"xPOkSw_uljYQ"},"source":["class ACReplayBuffer:\n","    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n","    def __init__(self, buffer_size=int(1e5), batch_size=64, seed=42, device='cpu'):\n","        \"\"\"Initialize a ReplayBuffer object.\n","        Params:\n","            buffer_size (int): maximum size of buffer\n","            batch_size (int): size of each training batch\n","            seed (int): random seed\n","            device (str): device where tensors are proecssed\n","        \"\"\"\n","\n","        self.memory = deque(maxlen=buffer_size)\n","        self.batch_size = batch_size\n","        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n","        self.device = device\n","        self.seed = random.seed(seed)\n","\n","    def add(self, state, action, reward, next_state, done):\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        e = self.experience(state, action, reward, next_state, done)\n","        self.memory.append(e)\n","\n","    def sample(self, batch_size=None):\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        batch_size = batch_size if batch_size is not None else self.batch_size\n","        batch = random.sample(self.memory, k=batch_size)\n","\n","        states = torch.from_numpy(np.vstack([b.state for b in batch if b is not None])).float().to(self.device)\n","        actions = torch.from_numpy(np.vstack([b.action for b in batch if b is not None])).long().to(self.device)\n","        rewards = torch.from_numpy(np.vstack([b.reward for b in batch if b is not None])).float().to(self.device)\n","        next_states = torch.from_numpy(np.vstack([b.next_state for b in batch if b is not None])).float().to(self.device)\n","        dones = torch.from_numpy(np.vstack([b.done for b in batch if b is not None]).astype(np.uint8)).float().to(self.device)\n","        return (states, actions, rewards, next_states, dones)\n","\n","    def is_ready_to_sample(self):\n","        return len(self) > self.batch_size\n","\n","    def __len__(self):\n","        \"\"\"Return the current size of internal memory.\"\"\"\n","        return len(self.memory)\n","\n","    def set_device(self, device):\n","        self.device = device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kyMyk4ETmOUF"},"source":["# Try to fill in data into buffer and sample batches from it.\n","game = \"BipedalWalker-v3\"\n","env = gym.make(game)\n","replay_buffer = ACReplayBuffer(batch_size=30)\n","\n","for _ in tqdm.tqdm(range(100)):\n","  state = env.reset()\n","  while True:\n","      action = env.action_space.sample()      \n","      next_state, reward, done, info = env.step(action)\n","      replay_buffer.add(state, action, reward, next_state, done)\n","      state = next_state\n","      if done: \n","        break;   \n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmUMJ8_Vu3Un"},"source":["replay_buffer.sample(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HPQR5BCKdNPp"},"source":["#### Actor Critic Networks"]},{"cell_type":"code","metadata":{"id":"blgu2InVEnd3"},"source":["def hidden_init(layer):\n","    fan_in = layer.weight.data.size()[0]\n","    lim = 1. / np.sqrt(fan_in)\n","    return (-lim, lim)\n","\n","\n","class Actor(torch.nn.Module):\n","    \"\"\"Actor (Policy) Model.\"\"\"\n","\n","    def __init__(self, state_size, action_size, seed):\n","        \"\"\"Initialize parameters and build model.\n","        Params\n","        ======\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","            seed (int): Random seed\n","        \"\"\"\n","\n","        super(Actor, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","\n","        self.fc1 = torch.nn.Linear(state_size, 256)\n","        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n","        #torch.nn.init.xavier_normal_(self.fc1.weight)\n","        \n","        self.fc2 = torch.nn.Linear(256, action_size)\n","        self.fc2.weight.data.uniform_(-3e-3, 3e-3)\n","\n","    def forward(self, state):\n","        x = torch.nn.functional.leaky_relu(self.fc1(state))\n","        return torch.tanh(self.fc2(x))\n","\n","\n","class Critic(torch.nn.Module):\n","    \"\"\"Critic (Value) Model.\"\"\"\n","\n","    def __init__(self, state_size, action_size, seed):\n","        \"\"\"Initialize parameters and build model.\n","        Params\n","        ======\n","            state_size (int): Dimension of each state\n","            action_size (int): Dimension of each action\n","            seed (int): Random seed\n","        \"\"\"\n","\n","        super(Critic, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","        \n","        self.fc1 = torch.nn.Linear(state_size, 256)\n","        #torch.nn.init.xavier_normal_(self.fc1.weight)\n","        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n","\n","        self.fc2 = torch.nn.Linear(256 + action_size, 256)\n","        #torch.nn.init.xavier_normal_(self.fc2.weight)\n","        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n","\n","        self.fc3 = torch.nn.Linear(256, 128)\n","        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n","\n","        self.fc4 = torch.nn.Linear(128, 1)\n","        self.fc4.weight.data.uniform_(-3e-3, 3e-3)\n","\n","\n","    def forward(self, state, action):\n","        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n","        x = torch.nn.functional.leaky_relu(self.fc1(state))\n","        x = torch.cat((x, action.float()), dim=1)\n","        x = torch.nn.functional.leaky_relu(self.fc2(x))\n","        x = torch.nn.functional.leaky_relu(self.fc3(x))\n","        return self.fc4(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lOOXoENgoZ5p"},"source":["states, actions, rewards, next_states, dones = replay_buffer.sample(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tEDvTprCoWkg"},"source":["actor = Actor(24, 4, seed=43)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbw8-5N6qk9G"},"source":["actor(states)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHW6PFKvq-qb"},"source":["critic = Critic(24, 4, seed=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZvvBNNEerH9n"},"source":["critic(states, actions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TxEhS1tIsBxs"},"source":["#### Agent"]},{"cell_type":"code","metadata":{"id":"3gRg4bEtsD5i"},"source":["class ACAgent():\n","    \"\"\"Interacts with and learns from the environment.\"\"\"\n","    def __init__(self, state_size, action_size, tau=1e-3, weight_decay=0.0001, gamma=0.99, lr_actor=1e-4, lr_critic=3e-4, seed=42, device='cpu'):\n","        \"\"\"Initialize an Agent object.\n","        Params:\n","            state_size (int): Dimension of state.\n","            action_size (int): Dimension of action.\n","            lr_actor (float): Learning rate for actor optimization.\n","            lr_critic (float): Learning rate for critic optimization.\n","            gamma (float): Reward discount,\n","            tau (float): For soft update of target network parameters.\n","            weight_decay (float): l2 loss during adam optimization.\n","        \"\"\"\n","        self.device = device\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        \n","        self.gamma = gamma\n","        self.tau = tau\n","        self.weight_decay = weight_decay\n","        self.lr_actor = lr_actor\n","        self.lr_critic = lr_critic\n","\n","        # Actor Network\n","        self.actor_local = Actor(state_size, action_size, seed).to(device)\n","        self.actor_target = Actor(state_size, action_size, seed).to(device)\n","        self.actor_optimizer = torch.optim.Adam(self.actor_local.parameters(), lr=self.lr_actor)\n","\n","        # Critic Network\n","        self.critic_local = Critic(state_size, action_size, seed).to(device)\n","        self.critic_target = Critic(state_size, action_size, seed).to(device)\n","        self.critic_optimizer = torch.optim.Adam(self.critic_local.parameters(), lr=self.lr_critic, weight_decay=self.weight_decay)\n","\n","        self.noise = OUNoise(action_size, seed)\n","\n","    def __repr__(self):\n","        return f'ACAgent(state_size={self.state_size}, action_size={self.action_size}, num_agents={self.num_agents}, device=\"{self.device}\")'\n","\n","\n","    def act(self, state, add_noise=True):\n","        \"\"\"Return actions for given state.\n","        Params:\n","            state (array_like): Current state.\n","            add_noise (bool): Add UO noise to actions.\n","        \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n","\n","        is_training = self.actor_local.training\n","        self.actor_local.eval()\n","        with torch.no_grad():\n","            actions = self.actor_local(state).cpu().data.numpy()[0]\n","        self.actor_local.train(is_training)\n","\n","        if add_noise:\n","            actions += self.noise.sample()\n","        return np.clip(actions, -1, 1)\n","\n","    def reset(self):\n","        self.noise.reset()\n","\n","    def train(self, replay_buffer, gamma=None, tau=None, batch_size=None):\n","        \"\"\"Update value parameters using sampled batches from replay buffer.\n","        Params:\n","            replay_buffer: Buffer with records from history.\n","            gamma (float): Discount factor.\n","            tau (float): For soft update of target network parameters\n","        \"\"\"\n","\n","        if tau is None:\n","            tau = self.tau\n","\n","        if gamma is None:\n","            gamma = self.gamma\n","\n","        if not replay_buffer.is_ready_to_sample():\n","            return None\n","        batch = replay_buffer.sample(batch_size)\n","        states, actions, rewards, next_states, dones = batch\n","\n","        # Use target actor to predict next continuous action. \n","        next_actions = self.actor_target(next_states)\n","        # Compute Q targets for current states.\n","        target = rewards + (gamma * self.critic_target(next_states, next_actions) * (1 - dones))\n","        prediction = self.critic_local(states, actions)\n","        # Compute critic loss.\n","        critic_loss = torch.nn.functional.mse_loss(prediction, target)\n","        # Minimize the loss.\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        # Compute actor loss.\n","        actions_pred = self.actor_local(states)\n","        actor_loss = -self.critic_local(states, actions_pred).mean()\n","        # Minimize the loss\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        self.soft_update(self.critic_local, self.critic_target, tau)\n","        self.soft_update(self.actor_local, self.actor_target, tau)\n","\n","\n","    def soft_update(self, local_model, target_model, tau=None):\n","        \"\"\"Soft update model parameters.\n","        θ_target = τ*θ_local + (1 - τ)*θ_target\n","        Params:\n","            local_model (PyTorch model): weights will be copied from\n","            target_model (PyTorch model): weights will be copied to\n","            tau (float): For soft update of target network parameters\n","        \"\"\"\n","        if tau is None:\n","            tau = self.tau\n","\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n","\n","    def save(self, path):\n","        prefix, _ = path.rsplit('.', 1)\n","        folder, file = path.rsplit('/', 1)\n","        if not os.path.exists(folder):\n","            os.makedirs(folder)\n","        torch.save(self.actor_local.state_dict(), prefix + '_actor.pth')\n","        torch.save(self.critic_local.state_dict(), prefix + '_critic.pth')\n","\n","    def load(self, path):\n","        prefix, _ = path.rsplit('.', 1)\n","        self.actor_local.load_state_dict(torch.load(prefix + '_actor.pth'))\n","        self.actor_target.load_state_dict(torch.load(prefix + '_actor.pth'))\n","        self.critic_local.load_state_dict(torch.load(prefix + '_critic.pth'))\n","        self.critic_target.load_state_dict(torch.load(prefix + '_critic.pth'))\n","\n","    def set_device(self, device):\n","      self.device = device\n","      self.actor_local = self.actor_local.to(device)\n","      self.actor_target = self.actor_target.to(device)\n","      self.critic_local = self.critic_local.to(device)\n","      self.critic_target = self.critic_target.to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HkN5GC7sEDe"},"source":["agent = ACAgent(24, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gy6SIdmk4fV6"},"source":["agent.act(np.random.randn(24))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xnf4dX_T4416"},"source":["#### Actor Critic Algorithm"]},{"cell_type":"code","metadata":{"id":"u52plq2C5naD"},"source":["#######################\n","# Setup of parameters #\n","#######################\n","\n","episodes = 2000                           # Number of episodes played.\n","steps_per_episodes = 700                  # Maximal amount of steps in one episode.\n","batch_size = 128                          # Size of batches sampled during training from replay buffer.\n","gamma = 0.99                              # Reward discounting.\n","stop_reward = 300                         # Average reward from 100 consecutive runs which would stop algorithm.\n","rewards_window = deque(maxlen=100)        # Buffer for 100 consecutive run rewards.        \n","rewards = []                              # Log of all episode rewards.\n","best_reward = -1000\n","model_name = 'bipedal'                    # Identifier of model saved params.\n","game = \"BipedalWalker-v3\"\n","seed = 42\n","\n","\n","env = gym.make(game)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","agent = ACAgent(state_size=24, action_size=4, gamma=gamma, lr_actor=1e-4, lr_critic=3e-4, seed=seed, device=device)\n","replay_buffer = ACReplayBuffer(buffer_size=int(1e5), batch_size=batch_size, seed=seed, device=device)\n","\n","\n","##################################\n","# Actor-Critic lerning algorithm #\n","##################################\n","\n","for episode_id in range(episodes):\n","    episode_reward = 0\n","    # At the start of episode, we restart the environment.\n","    state = env.reset()\n","    # Here starts episode.\n","    for t_step in range(steps_per_episodes):\n","        # Agent selects action.\n","        action = agent.act(state, add_noise=True)\n","        next_state, reward, done, info = env.step(action)\n","        # Save experience into replay buffer.\n","        replay_buffer.add(state, action, reward, next_state, done)\n","        # Train with train_rate.\n","        agent.train(replay_buffer, gamma=gamma, batch_size=batch_size)          \n","        episode_reward += reward\n","        if done:\n","            break\n","        state = next_state\n","    ##########################################################################\n","    # Experiment with lowering the noise ration after few 100s of iterations #\n","    ##########################################################################\n","    #agent.reset()\n","\n","\n","    # Reporting.\n","    rewards_window.append(episode_reward)\n","    rewards.append(episode_reward)\n","    print(f'\\rEpisode {episode_id}\\tAverage Score: {np.round(np.mean(rewards_window), 1)}\\t Current Score: {np.round(episode_reward, 1)}', end=\"\")\n","    if best_reward <= episode_reward:\n","      best_reward = episode_reward\n","      agent.save(path=os.path.join(rl_workshop_path, f'models/{model_name}-best-{np.round(np.mean(rewards_window), 1)}-{np.round(episode_reward, 1)}.pth'))\n","    if episode_id % 100 == 0:\n","        print(f'\\rEpisode {episode_id}\\tAverage Score: {np.round(np.mean(rewards_window), 1)}')\n","        agent.save(path=os.path.join(rl_workshop_path, f'models/{model_name}-{episode_id}-{np.round(np.mean(rewards_window), 1)}-{np.round(episode_reward, 1)}.pth'))\n","    if np.mean(rewards_window)>=stop_reward:\n","        print(f'\\nSolved! Took {episode_id-100} episodes\\tAverage Score: {np.round(np.mean(rewards_window), 1)}')\n","        agent.save(path=os.path.join(rl_workshop_path, f'models/{model_name}-{episode_id}-{np.round(np.mean(rewards_window), 1)}-{np.round(episode_reward, 1)}.pth'))\n","        break\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jd27CbKz5niT"},"source":["env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rs8TODBz5nk7"},"source":["fig = plt.figure(figsize=(10, 10))\n","plt.plot(np.arange(len(rewards)), rewards)\n","plt.ylabel('Reward')\n","plt.xlabel('Episode')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5trtmZ5e57ZK"},"source":["!ls '/content/drive/My Drive/ml_college_data/rl_workshop/models/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oZUFtGSY5_PE"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","agent = ACAgent(state_size=24, action_size=4, gamma=0.99, lr_actor=0.0001, lr_critic=0.0001, seed=42, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DpduzPn5_Ry"},"source":["model_path = os.path.join(rl_workshop_path, 'models/bipedal-best--80.5-2.2.pth')\n","video_path = '/video/bipedal/'\n","game = \"BipedalWalker-v3\"\n","\n","agent.load(path=model_path)\n","env = wrap_env(gym.make(game), video_path)\n","state = env.reset()\n","while True:\n","    env.render('human')\n","    action = agent.act(state, add_noise=True)      \n","    state, reward, done, info = env.step(action) \n","    if done: \n","      break;   \n","env.close()\n","show_video(video_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"153oUaq6E4Yh"},"source":[" #### Questions and experiment suggestions\n"," - Try different Gaussian Markov process for randomization of actions\n"," - Go hardcore https://gym.openai.com/envs/BipedalWalkerHardcore-v2/\n"," - Go back to reinforce algorithm and apply critic each step instead of R at the end of episode\n"," - Use 2 headed policy-value network \n"]}]}